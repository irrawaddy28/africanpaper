
IEEEexample.bib 
V1.12 (2007/01/11)
Copyright (c) 2002-2007 by Michael Shell
See: http://www.michaelshell.org/
for current contact information.

This is an example BibTeX database for the official IEEEtran.bst
BibTeX style file.

Some entries call strings that are defined in the IEEEabrv.bib file.
Therefore, IEEEabrv.bib should be loaded prior to this file. 
Usage: 

\bibliographystyle{./IEEEtran}
\bibliography{./IEEEabrv,./IEEEexample}


Support sites:
http://www.michaelshell.org/tex/ieeetran/
http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
and/or
http://www.ieee.org/

*************************************************************************
Legal Notice:
This code is offered as-is without any warranty either expressed or
implied; without even the implied warranty of MERCHANTABILITY or
FITNESS FOR A PARTICULAR PURPOSE! 
User assumes all risk.
In no event shall IEEE or any contributor to this code be liable for
any damages or losses, including, but not limited to, incidental,
consequential, or any other damages, resulting from the use or misuse
of any information contained here.

All comments are the opinions of their respective authors and are not
necessarily endorsed by the IEEE.

This work is distributed under the LaTeX Project Public License (LPPL)
( http://www.latex-project.org/ ) version 1.3, and may be freely used,
distributed and modified. A copy of the LPPL, version 1.3, is included
in the base LaTeX documentation of all distributions of LaTeX released
2003/12/01 or later.
Retain all contribution notices and credits.
** Modified files should be clearly indicated as such, including  **
** renaming them and changing author support contact information. **

File list of work: IEEEabrv.bib, IEEEfull.bib, IEEEexample.bib,
                   IEEEtran.bst, IEEEtranS.bst, IEEEtranSA.bst,
                   IEEEtranN.bst, IEEEtranSN.bst, IEEEtran_bst_HOWTO.pdf
*************************************************************************

MLLR
@article{Leggetter-MLLR,
title = "Maximum likelihood linear regression for speaker adaptation of continuous density hidden {M}arkov models ",
journal = "Computer Speech \& Language ",
volume = "9",
number = "2",
pages = "171 - 185",
year = "1995",
note = "",
author = "C.J. Leggetter and P.C. Woodland",
}

MLLT
@inproceedings{Gopinath-MLLT,
  title			= "Maximum likelihood modeling with {G}aussian distributions for classification",
  author		= "R. Gopinath",
  booktitle		= ICASSP,  
  pages			= "661-664",
  year			= "1998",  
}

CMLLR or fMLLR
@article{Gales-CMLLR,
  author        = "M. J. F. Gales",
  title		= "Maximum Likelihood Linear Transformations for {HMM}-based Speech Recognition",
  journal       = J_CSL,
  volume        = "12",
  number        = "",
  month         = "",
  year          = "1997",
  pages         = "75-98"
}

First paper on CD-DNN-HMM 
@article{Dahl-CD-DNN-HMM,
  author    = {George E. Dahl and  Dong Yu and   Li Deng and Alex Acero},
  title     = {Context-Dependent Pre-Trained Deep Neural Networks for Large-Vocabulary  Speech Recognition},
  journal   = IEEE_J_ASL,
  volume    = {20},
  number    = {1},
  pages     = {30-42},
  month     = {Jan},
  year      = {2012}, 
}

Acoustic modeling of context-independent phones using DBN 
@article{Mohamed-DBN,
author={Mohamed, A. and Dahl, G.E. and Hinton, G.},
journal=IEEE_J_ASL,
title={Acoustic Modeling Using Deep Belief Networks},
volume={20},
number={1},
pages={14-22},
month={Jan},
year={2012},
}


@inproceedings{Yu-FeatureLearning,
  title			= "Feature Learning in Deep Neural Networks - Studies on Speech Recognition Tasks",
  author		= "D. Yu and M. L. Seltzer and J. Li and J-T. Huang and F. Seide",
  booktitle		= "http://arxiv.org/abs/1301.3605",  
  pages			= "",
  year			= "2013",  
}


@inproceedings{Thomas-DNNPostFeatures,
  author = {S. Thomas and S. Ganapathy and H. Hermansky},
  booktitle = {Interspeech},  
  title = {Cross-lingual and Multi-stream Posterior Features for Low Resource {LVCSR} Systems},
  year = 2010
}


Bottleneck features
@inproceedings{Grezl-BNFFirstPaper,
  title			= "Probabilistic and bottle-neck features for {LVCSR} of meetings",
  author		= "F. Gr\'{e}zl and M. Karafi\'{a}t and S. Kont\'{a}r and J. \v{C}ernock\'{y}",
  booktitle		= ICASSP,  
  year			= "2007",  
}


BNFs for xlingual scenarios
@inproceedings{Thomas-BNFXlingual,
  title			= "Multilingual {MLP} features for low-resource {LVCSR} systems",
  author		= "S. Thomas and S. Ganapathy and H. Hermansky",
  booktitle		= ICASSP,  
  year			= "2012",  
}



First to propose the multiple softmax style architecture for training an ANN simultaneously using data from multiple languages. Their ANN was used as a front-end discriminant features generator that were later used to train language dependent hidden Markov model (HMM) based speech recognizer
@inproceedings{Scanzio-MultisoftmaxFirstPaper,
  title			= "On the Use of a Multilingual Neural Network Front-End",
  author		= "S. Scanzio and P. Laface and L. Fissore and R. Gemello and F. Mana ",
  booktitle		= {Interspeech},  
  year			= "2008",
  pages			= "2711â€“2714",
}

Multilingual SHLs perform better than monolingual SHLs
@inproceedings{Huang-MultilingualSHL,
  title			= "Cross language knowlege transfer using multilingual deep neural network with shared hidden layers",
  author		= "J-T. Huang and J. Li and D. Yu and L. Deng and Y. Gong ",
  booktitle		= ICASSP,  
  year			= "2013",  
}


Sequential training of multilingual DNN
@inproceedings{Ghoshal-MultilingualPretraining,
  title			= "Multilingual training of deep neural networks",
  author		= "A. Ghoshal and P. Swietojanski and S. Renals",
  booktitle		= ICASSP,  
  year			= "2013",  
}

MLP Initialization Schemes (open target MLP)
@inproceedings{Vu-MLPInitSchemes,
  title			= "An investigation on initialization schemes for multilayer perceptron training using multilingual data and their effect on {ASR} performance",
  author		= "N. Vu and W. Breiter and F. Metze and T. Schultz",
  booktitle		= {Interspeech},  
  year			= "2012",  
}

Two techniques for zero resource a) Self training: Use an existing multilingual AM to decode unsup data in target language. Then use the decoded transcriptions to adapt the multilingual AM to the target language b) Use audio segments (with high confidences obtained from decoded transcriptions) to pretrain DBN and then do x-ent training using the high confidence labels.
@inproceedings{Knill-SelfTrainingAndUnsupAdapt,
  title			= "Language independent	and unsupervised acoustic models for speech recognition and keyword spotting",
  author		= "K. Knill and M. J. F. Gales and A. Ragni and S. Rath",
  booktitle		= {Interspeech},  
  year			= "2014",  
}

Using Restricted Boltzmann Machine (RBM) pre-training, the DNNs were initialized in a greedy layer-wise fashion since this has been shown to yield better local minima over random initialization
@inproceedings{Bengio-Pretraining,
  title			= "Greedy layer-wise training of deep networks",
  author		= "Y. Bengio and P. Lamblin and D. Popovici and H. Larochelle",
  booktitle		= {Adv. in Neural Information Processing Systems},  
  year			= "2006",  
}


Semisupervised training of deep neural networks
@inproceedings{Vesely-SemisupTrainingDNN,
  title			= "Semi-supervised training of deep neural networks",
  author		= "K. Vesely and M. Hannemann and L. Burget",
  booktitle		= IEEEASRU, 
  pages     		= {267-272}, 
  year			= "2013",  
}

MAP-HMM for PT
@inproceedings{Liu-PTAdaptedGMM,
  title			= "Adapting {ASR} for under-resourced languages using mismatched transcriptions",
  author		= "C. Liu and P. Jyothi and H. Tang and V. Manohar and R. Sloan and T. Kekona and M. Hasegawa-Johnson and S. Khudanpur",
  booktitle		= ICASSP,  
  year			= "2016",  
}

Kaldi Toolkit
@inproceedings{Povey-Kaldi,
  title			= "The {K}aldi speech recognition toolkit",
  author		= "D. Povey and A. Ghoshal and G. Boulianne and L. Burget and O. Glembek and 
N. Goel and M. Hannemann and P. Motl\'{i}\v{c}ek and Y. Qian and P. Schwarz and J. Silovsk\'{y} and G. Stemmer and K. Vesel\'{y}",
  booktitle		= IEEEASRU,  
  year			= "2011",  
}


WFST
@article{Mohri-WFST,
  author        = "M. Mohri, F. Pereira, and M. Riley",
  title		= "Weighted finite-state transducers in speech recognition",
  journal       = J_CSL,
  volume        = "20",
  number        = "1",
  month         = "",
  year          = "2002",
  pages         = "69-88"
}


Mismatched Crowdsourcing
@inproceedings{Jyothi-MismatchedCrowdsourcingTrans,
  title			= "Transcribing continuous speech using mismatched crowdsourcing",
  author		= "P. Jyothi and M. Hasegawa-Johnson",
  booktitle		= {Interspeech},  
  year			= "2015",  
}

@inproceedings{Jyothi2015,
    Author = {Jyothi, Preethi and Hasegawa-Johnson, Mark},
    Booktitle = {AAAI},
    Title = {Acquiring Speech Transcriptions Using Mismatched Crowdsourcing},
    Year = {2015}
}

DNN Adapted with PTs
@inproceedings{Das-PTAdaptedDNN,
  title			= "An investigation on training deep neural networks using probabilistic transcriptions",
  author		= "A. Das and M. Hasegawa-Johnson",
  booktitle		= {Interspeech},  
  year			= "2016",  
}

Swahili ASR
@inproceedings{Gelas-SwahiliASR,
  title			= "Developments of Swahili resources for an automatic speech recognition system",
  author		= "H. Gelas and L. Besacier and F. L. Pellegrino",
  booktitle		= {SLTU-Workshop on Spoken Language Technologies for Under-Resourced Languages},  
  year			= "2012",  
}

Amharic ASR
@article{Tachbelie2014,
 Author = {Martha Yifiru Tachbelie and Solomon Teferra Abate and
  Laurent Besacier},
 Journal = jsc,
 Pages = {181 -- 194},
 Publisher = {Elsevier},
 Title = {Using different acoustic, lexical and language modeling units for
  {ASR} of an under-resourced language -- {Amharic}},
 Volume = {56},
 Year = {2014}
 }

Dinka phonology
@article{Remijsen-LuanyjangDinka,
 title="Luanyjang Dinka",
 author= "B. Remijsen and C. A. Manyang",
 journal= {Journal of the International Phonetic Association},
 keywords="", 
 volume="39",
 number="1",
 pages="113-124",
 year="2009",
 month="",
}
 
@electronic{DinkaOmniglot, 
 title     = "Dinka Omniglot",
 url       = "http://www.omniglot.com/writing/dinka.php"
}

@electronic{SBS, 
 title     = "{Special Broadcasting Service (SBS)}",
 url       = "http://www.sbs.com.au/yourlanguage"
}

@unpublished{MarkG2P,
    Author = {Mark Hasegawa-Johnson},
    Note = {Downloaded 9/24/2015 from http://isle.illinois.edu/sst/data/dict},
    Title = {{WS15} Dictionary Data},
    Year = {2015}
}

@electronic{DinkaWiki, 
 title     = "Dinka Language Wiki",
 url       = "https://en.wikipedia.org/wiki/Dinka_language"
}



An example of a IEEEtran control entry which can change some IEEEtran.bst
settings. An entry like this must be cited via \bstctlcite{} command
before the first real \cite{}. The same entry key cannot be called twice
(just like multiple \cite{} of the same entry key place only one entry
in the bibliography.)
The available control fields are:

CTLuse_article_number
"no" turns off the display of the number for articles.
"yes" enables

CTLuse_paper
"no" turns off the display of the paper and type fields in inproceedings.
"yes" enables

CTLuse_forced_etal 
"no" turns off the forced use of "et al."
"yes" enables

CTLmax_names_forced_etal
The maximum number of names that can be present beyond which an "et al."
usage is forced. Be sure that CTLnames_show_etal (below)
is not greater than this value!

CTLnames_show_etal
The number of names that will be shown with a forced "et al.".
Must be less than or equal to CTLmax_names_forced_etal

CTLuse_alt_spacing 
"no" turns off the alternate interword spacing for entries with URLs.
"yes" enables

CTLalt_stretch_factor
If alternate interword spacing for entries with URLs is enabled, this is
the interword spacing stretch factor that will be used. For example, the
default "4" here means that the interword spacing in entries with URLs can
stretch to four times normal. Does not have to be an integer.

CTLdash_repeated_names
"no" turns off the "dashification" of repeated (i.e., identical to those
of the previous entry) names. IEEE normally does this.
"yes" enables

CTLname_format_string
The name format control string as explained in the BibTeX style hacking
guide.
IEEE style "{f.~}{vv~}{ll}{, jj}" is the default,

CTLname_latex_cmd
A LaTeX command that each name will be fed to (e.g., "\textsc").
Leave empty if no special font is desired for the names.
The default is empty.

CTLname_url_prefix
The prefix text used before URLs.
The default is "[Online]. Available:" A space will be inserted after this
text. If this space is not wanted, just use \relax at the end of the
prefix text.


Those fields that are not to be changed can be left out.
@IEEEtranBSTCTL{IEEEexample:BSTcontrol,
  CTLuse_article_number     = "yes",
  CTLuse_paper              = "yes",
  CTLuse_forced_etal        = "no",
  CTLmax_names_forced_etal  = "10",
  CTLnames_show_etal        = "1",
  CTLuse_alt_spacing        = "yes",
  CTLalt_stretch_factor     = "4",
  CTLdash_repeated_names    = "yes",
  CTLname_format_string     = "{f.~}{vv~}{ll}{, jj}",
  CTLname_latex_cmd         = "",
  CTLname_url_prefix        = "[Online]. Available:"
}


